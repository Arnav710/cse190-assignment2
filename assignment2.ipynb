{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "### FILL IN CONTEXT FOR THE DATASET HERE\n",
    "Where does your dataset come from? What is it for, how was it\n",
    "collected, etc.?\n",
    "\n",
    "FILL HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK\n",
    "Code: Report standard accuracy and fairness metrics of labels and predictions,\n",
    "such as those in Modules 1 and 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaravtiku/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6282\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"diabetic_data.csv\") \n",
    "\n",
    "# converting hospital readmission data into binary - 1 if visited in less than 30 days, 0 if false.\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x in ['<30', '>30'] else 0)\n",
    "\n",
    "# Dropping irrelevant columns\n",
    "# encounter_id, patient_nbr -> Unique identifier of an encounter, and patient number, which seem irrelevant for prediction tasks\n",
    "# weight -> 97% of weight entries missing\n",
    "# readmitted -> we make a new binary y_hat label as indicated in the line above to deal with readmitted patients\n",
    "# payer_code , medical_specialty -> not enough conclusive data to make decision, i.e. >50% of entries\n",
    "\n",
    "X = data.drop(columns=['encounter_id', 'patient_nbr', 'readmitted', 'payer_code', 'weight', 'medical_specialty'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "num_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# replaces missing numerical columns with observed mean values\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy=\"mean\")),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# using OHE, and converting missing entries into the most_frequent category\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_transformer, num_features),\n",
    "    ('cat', cat_transformer, cat_features)\n",
    "])\n",
    "\n",
    "# Create multi-layered logistic regression pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# OVERALL ACCURACY -> Metric 1\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.6256\n",
      "Recall Score: 0.4861\n",
      "F1 Score: 0.5471\n",
      "Mean Squared Error (MSE): 0.3718\n",
      "Mean Absolute Error (MAE): 0.3718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision Score: {precision:.4f}\")\n",
    "print(f\"Recall Score: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Group [70-80): 20879 entries in X_train\n",
      "Age Group [50-60): 13741 entries in X_train\n",
      "Age Group [80-90): 13737 entries in X_train\n",
      "Age Group [40-50): 7810 entries in X_train\n",
      "Age Group [60-70): 17959 entries in X_train\n",
      "Age Group [30-40): 3030 entries in X_train\n",
      "Age Group [10-20): 551 entries in X_train\n",
      "Age Group [90-100): 2239 entries in X_train\n",
      "Age Group [20-30): 1335 entries in X_train\n",
      "Age Group [0-10): 131 entries in X_train\n"
     ]
    }
   ],
   "source": [
    "# checking across race, gender, age\n",
    "unique_age_groups = X_train['age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16666666666666666, 0.40714285714285714, 0.48757763975155277, 0.4389261744966443, 0.4464, 0.4415362731152205, 0.46595932802829354, 0.47831952206590866, 0.47398843930635837, 0.427797833935018]\n",
      "Number of p% rule violations: 9\n",
      "Positive parity value of age Group [30-40): 0.7149532710280374\n",
      "Positive parity value of age Group [70-80): 0.6052505966587112\n",
      "Positive parity value of age Group [50-60): 0.6522148916116871\n",
      "Positive parity value of age Group [80-90): 0.5619946091644205\n",
      "Positive parity value of age Group [40-50): 0.6918918918918919\n",
      "Positive parity value of age Group [60-70): 0.6440576230492197\n",
      "Positive parity value of age Group [20-30): 0.9029126213592233\n",
      "Positive parity value of age Group [90-100): 0.5596330275229358\n",
      "Positive parity value of age Group [10-20): 0.6666666666666666\n",
      "Negative parity value of age Group [70-80): 0.6076276664511958\n",
      "Negative parity value of age Group [50-60): 0.6495517522412388\n",
      "Negative parity value of age Group [80-90): 0.5921052631578947\n",
      "Negative parity value of age Group [40-50): 0.6568181818181819\n",
      "Negative parity value of age Group [30-40): 0.672316384180791\n",
      "Negative parity value of age Group [60-70): 0.6378586424072779\n",
      "Negative parity value of age Group [20-30): 0.7077625570776256\n",
      "Negative parity value of age Group [90-100): 0.604494382022472\n",
      "Negative parity value of age Group [10-20): 0.6311475409836066\n",
      "Negative parity value of age Group [0-10): 0.8333333333333334\n",
      "Equal opportunity value of age Group [30-40): 0.7149532710280374\n",
      "Equal opportunity value of age Group [70-80): 0.6052505966587112\n",
      "Equal opportunity value of age Group [50-60): 0.6522148916116871\n",
      "Equal opportunity value of age Group [80-90): 0.5619946091644205\n",
      "Equal opportunity value of age Group [40-50): 0.6918918918918919\n",
      "Equal opportunity value of age Group [60-70): 0.6440576230492197\n",
      "Equal opportunity value of age Group [20-30): 0.9029126213592233\n",
      "Equal opportunity value of age Group [90-100): 0.5596330275229358\n",
      "Equal opportunity value of age Group [10-20): 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Treating age as the sensitive attribute\n",
    "unique_age_groups = data['age'].unique()\n",
    "\n",
    "from collections import defaultdict\n",
    "age_counts = defaultdict(int)  \n",
    "positive_label_counts = defaultdict(int) \n",
    "\n",
    "for age, lbl in zip(X_test['age'], y_test):  \n",
    "    age_counts[age] += 1 \n",
    "    if lbl == 1:\n",
    "        positive_label_counts[age] += 1  \n",
    "\n",
    "demographic_parity = []\n",
    "for i in unique_age_groups:\n",
    "    demographic_parity.append(positive_label_counts[i]/age_counts[i])\n",
    "\n",
    "# DEMOGRAPHIC PARITY / ACCURACY PARITY??? Slides have the same formula for both\n",
    "print(demographic_parity)\n",
    "\n",
    "# p% rule\n",
    "count_rule_violation = 0\n",
    "\n",
    "for i in range(len(demographic_parity)):\n",
    "    for j in range(i + 1, len(demographic_parity)):\n",
    "        if demographic_parity[i] / demographic_parity[j] < 0.8:\n",
    "            count_rule_violation += 1\n",
    "\n",
    "print(f\"Number of p% rule violations: {count_rule_violation}\")\n",
    "\n",
    "# PPV and NPV\n",
    "\n",
    "negative_outcomes = defaultdict(int)\n",
    "positive_outcomes = defaultdict(int)\n",
    "negative_predn = defaultdict(int)\n",
    "positive_predn = defaultdict(int)\n",
    "\n",
    "for idx, label in enumerate(y_test):\n",
    "    prediction = y_pred[idx]\n",
    "    age_group = X_test.iloc[idx]['age']\n",
    "    if prediction == label and label == 0:\n",
    "        negative_outcomes[age_group] += 1\n",
    "    elif prediction == label and label == 1:\n",
    "        positive_outcomes[age_group] += 1\n",
    "    if prediction == 0:\n",
    "        negative_predn[age_group] += 1\n",
    "    elif prediction == 1:\n",
    "        positive_predn[age_group] += 1\n",
    "\n",
    "for group in positive_outcomes:\n",
    "    if positive_predn[group] > 0:\n",
    "        ratio = positive_outcomes[group] / positive_predn[group]\n",
    "    print(f\"Positive parity value of age Group {group}: {ratio}\")\n",
    "\n",
    "for group in negative_outcomes:\n",
    "    if negative_predn[group] > 0:\n",
    "        ratio = negative_outcomes[group] / negative_predn[group]\n",
    "    print(f\"Negative parity value of age Group {group}: {ratio}\")\n",
    "\n",
    "\n",
    "# Equal Opportunity\n",
    "\n",
    "positive_outcomes = defaultdict(int)\n",
    "positive_outcomes_for_age_group = defaultdict(int)\n",
    "\n",
    "for idx, label in enumerate(y_test):\n",
    "    prediction = y_pred[idx]\n",
    "    age_group = X_test.iloc[idx]['age']\n",
    "    if prediction == 1 and label == 1:\n",
    "        positive_outcomes[age_group] += 1\n",
    "    if prediction == 1:\n",
    "        positive_outcomes_for_age_group[age_group] += 1\n",
    "\n",
    "for group in positive_outcomes:\n",
    "    ratio = positive_outcomes[group] / positive_outcomes_for_age_group[group]\n",
    "    print(f\"Equal opportunity value of age Group {group}: {ratio}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "Discuss which fairness metrics are specifically relevant to your task,\n",
    "e.g. accuracy parity might be less appropriate for recidivism prediction than\n",
    "demographic parity (etc.)\n",
    "\n",
    "FILL HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK\n",
    "\n",
    "How much can “unfairness” in your predictions be explained by dataset\n",
    "characteristics? Can you fix them with dataset-based interventions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK\n",
    "\n",
    "How do different modeling choices impact fairness characteristics? Can\n",
    "you fix them with in-processing interventions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK\n",
    "\n",
    "Can you apply post-processing interventions to achieve desired fairness outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK \n",
    "\n",
    "Discussion: What types of interventions are most appropriate for your task (e.g.\n",
    "legal, practical to deploy, etc.)? What are the tradeoffs between them (e.g. how\n",
    "are other metrics negatively impacted by a particular intervention, etc.)\n",
    "\n",
    "FILL HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK\n",
    "\n",
    "Implement a fairness intervention described in a research paper. Some\n",
    "possibilities include examples from class\n",
    "\n",
    "Additionally, Attempt to reproduce results similar to those reported in the paper on your\n",
    "dataset (or comment in detail about any failure to do so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK\n",
    "\n",
    "Summarize the main contributions of the paper and its relevance to\n",
    "your task\n",
    "\n",
    "FILL HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK\n",
    "\n",
    "Is it more effective than other intervention strategies you tried? Why\n",
    "or why not? Conclude your presentation with a general discussion of what was\n",
    "and was not effective for your task.\n",
    "\n",
    "FILL HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
