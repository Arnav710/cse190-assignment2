{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset (as in your attached file)\n",
    "data = pd.read_csv(\"job_applicant_data.csv\")\n",
    "data = data.drop(columns=['Unnamed: 0'])\n",
    "data['NumTechWorkedWith'] = data['HaveWorkedWith'].str.count(';') + 1\n",
    "data = data.drop(columns=['HaveWorkedWith'])\n",
    "\n",
    "# Encode sensitive and other attributes\n",
    "gender_mapping = {'Man': 1, 'Woman': 0, 'NonBinary': 2}\n",
    "inv_gender_mapping = {v: k for k, v in gender_mapping.items()}\n",
    "data['Gender'] = data['Gender'].map(gender_mapping)\n",
    "\n",
    "age_mapping = {'>35': 1, '<35': 0}\n",
    "inv_age_mapping = {v: k for k, v in age_mapping.items()}\n",
    "data['Age'] = data['Age'].map(age_mapping)\n",
    "\n",
    "ed_level_mapping = {'PhD': 4, 'Master': 3, 'Undergraduate': 2, 'NoHigherEd': 1, 'Other': 0}\n",
    "inv_ed_level_mapping = {v: k for k, v in ed_level_mapping.items()}\n",
    "data['EdLevel'] = data['EdLevel'].map(ed_level_mapping)\n",
    "\n",
    "label_encoders = {}\n",
    "for col in ['Age', 'Accessibility', 'MentalHealth', 'MainBranch', 'Country']:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col].astype(str))\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features, target, and sensitive attribute (Gender)\n",
    "X = data.drop(columns=['Employed'])\n",
    "y = data['Employed']\n",
    "S = data[\"Gender\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test, S_train, S_test = train_test_split(\n",
    "    X, y, S, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train_W_Gender = X_train.copy()\n",
    "X_test_W_Gender = X_test.copy()\n",
    "# Remove Gender from training features\n",
    "X_train = X_train.drop(columns=['Gender'])\n",
    "X_test = X_test.drop(columns=['Gender'])\n",
    "\n",
    "X_train_np = X_train.values\n",
    "X_test_np = X_test.values\n",
    "y_train = y_train.values.astype(float)\n",
    "y_test = y_test.values.astype(float)\n",
    "n_train = len(y_train)\n",
    "n_test = len(y_test)\n",
    "\n",
    "# Initialize predictor F (in log-odds space) as a constant function\n",
    "p_mean = np.mean(y_train)\n",
    "F_train = np.full(n_train, np.log(p_mean / (1 - p_mean)))\n",
    "F_test  = np.full(n_test,  np.log(p_mean / (1 - p_mean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the adversary network (a small neural network)\n",
    "class Adversary(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Adversary, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 16)   # Input: predictor's output (scalar)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 3)    # Output: logits for 3 classes (0,1,2)\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No softmax here; CrossEntropyLoss applies it internally\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adv_model = Adversary()\n",
    "adv_optimizer = optim.Adam(adv_model.parameters(), lr=0.01)\n",
    "adv_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Parameters for binning the combined residual u\n",
    "num_bins = 10  # number of bins\n",
    "# We will determine bin edges dynamically from u at each iteration\n",
    "\n",
    "# Boosting loop with adversarial correction (minâ€“max formulation) using RandomForestClassifier as weak learner\n",
    "lambda_fairness = 0.015  # Trade-off parameter\n",
    "M = 50                   # Number of boosting iterations\n",
    "alpha = 1.0              # Step size for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range(M):\n",
    "    # Compute current predicted probabilities using the sigmoid function\n",
    "    p_pred = sigmoid(F_train)\n",
    "    # Standard pseudoresiduals for logistic loss: r = y - p_pred\n",
    "    r = y_train - p_pred\n",
    "\n",
    "    # Compute adversary gradient: convert F_train to a torch tensor with gradients enabled\n",
    "    F_tensor = torch.tensor(F_train.reshape(-1, 1), dtype=torch.float32, requires_grad=True)\n",
    "    p_tensor = torch.sigmoid(F_tensor)\n",
    "    adv_logits = adv_model(p_tensor)\n",
    "    s_tensor = torch.tensor(S_train.values, dtype=torch.long)\n",
    "    adv_loss = adv_criterion(adv_logits, s_tensor)\n",
    "    adv_loss.backward()\n",
    "    t = F_tensor.grad.detach().numpy().flatten()\n",
    "\n",
    "    # Combine the gradients: u = r - lambda_fairness * t\n",
    "    u = r - lambda_fairness * t\n",
    "\n",
    "    # --- Using RandomForestClassifier as weak learner ---\n",
    "    bin_edges = np.linspace(np.min(u), np.max(u), num_bins+1)\n",
    "    # Digitize u: bin indices from 1 to num_bins; subtract 1 to have indices 0..num_bins-1\n",
    "    u_bins = np.digitize(u, bin_edges) - 1\n",
    "    # For regression approximation, we take the midpoint of each bin as the representative value.\n",
    "    bin_midpoints = (bin_edges[:-1] + bin_edges[1:]) / 2.0\n",
    "\n",
    "    # Train a RandomForestClassifier to predict the bin index from X_train_np.\n",
    "    rf_weak = RandomForestClassifier(n_estimators=10, max_depth=3, random_state=42)\n",
    "    rf_weak.fit(X_train_np, u_bins)\n",
    "    # Predict bin indices for training and test sets.\n",
    "    u_bins_pred_train = rf_weak.predict(X_train_np)\n",
    "    u_bins_pred_test = rf_weak.predict(X_test_np)\n",
    "    # Convert predicted bin indices to continuous values using the midpoints.\n",
    "    h_train = np.array([bin_midpoints[i] for i in u_bins_pred_train])\n",
    "    h_test = np.array([bin_midpoints[i] for i in u_bins_pred_test])\n",
    "\n",
    "    # Update the predictor F by adding the scaled weak learner prediction\n",
    "    F_train = F_train + alpha * h_train\n",
    "    F_test  = F_test  + alpha * h_test\n",
    "\n",
    "    # Update the adversary network using the new predictor outputs\n",
    "    F_tensor_new = torch.tensor(F_train.reshape(-1, 1), dtype=torch.float32)\n",
    "    p_tensor_new = torch.sigmoid(F_tensor_new)\n",
    "    adv_logits_new = adv_model(p_tensor_new)\n",
    "    adv_loss_new = adv_criterion(adv_logits_new, s_tensor)\n",
    "    adv_optimizer.zero_grad()\n",
    "    adv_loss_new.backward()\n",
    "    adv_optimizer.step()\n",
    "\n",
    "    # Monitoring: compute and print the predictor loss on the training set\n",
    "    pred_loss = -np.mean(y_train * np.log(p_pred + 1e-8) + (1 - y_train) * np.log(1 - p_pred + 1e-8))\n",
    "    print(f\"Iteration {m}: Predictor Loss = {pred_loss:.4f}, Adversary Loss = {adv_loss_new.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final predictions and evaluation on the test set\n",
    "final_prob_test = sigmoid(F_test)\n",
    "final_pred_test = (final_prob_test > 0.5).astype(int)\n",
    "test_acc = accuracy_score(y_test, final_pred_test)\n",
    "print(\"\\nFinal Test Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairness metrics calculation\n",
    "def compute_demographic_parity(predictions, sensitive, group):\n",
    "    idx = (sensitive == group)\n",
    "    return np.mean(predictions[idx])\n",
    "\n",
    "print(\"\\nDemographic Parity on Test Set:\")\n",
    "for group in np.unique(S_test.values):\n",
    "    rate = compute_demographic_parity(final_pred_test, S_test.values, group)\n",
    "    print(f\"Group {group} ({inv_gender_mapping[group]}): Positive Rate = {rate:.4f}\")\n",
    "\n",
    "print(\"\\nEqual Opportunity (True Positive Rate) on Test Set:\")\n",
    "for group in np.unique(S_test.values):\n",
    "    group_idx = (S_test.values == group)\n",
    "    positive_idx = (y_test == 1)\n",
    "    group_positive = group_idx & positive_idx\n",
    "    if np.sum(group_positive) > 0:\n",
    "        tpr = np.sum(final_pred_test[group_positive] == 1) / np.sum(group_positive)\n",
    "        print(f\"Group {group} ({inv_gender_mapping[group]}): TPR = {tpr:.4f}\")\n",
    "    else:\n",
    "        print(f\"Group {group} ({inv_gender_mapping[group]}): No positive samples\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
